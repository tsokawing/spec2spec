{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spec2spec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqGxxjZM3VkPTdt2whIWjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsokawing/spec2spec/blob/main/training/spec2spec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is an attempt to train a spec2spec model, which transforms audio from one style to another using their spectrogram images. Some of the codes here is based on the TensorFlow tutorial notebook on pix2pix."
      ],
      "metadata": {
        "id": "I5A6vloqc7Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "WuQAXnwuMrde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import setups:"
      ],
      "metadata": {
        "id": "qiEl47xFwL49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "PLvlaET1KKdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive:"
      ],
      "metadata": {
        "id": "NWa08ChPwTFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will load training data from Google Drive.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RQCjcLHidtes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions:"
      ],
      "metadata": {
        "id": "UNJT4KABwXGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spec(audio, samplerate, title='Mel-frequency Spectrogram', save=False):\n",
        "  fig, ax = plt.subplots()\n",
        "  audio_db = librosa.power_to_db(audio, ref=np.max)\n",
        "  img = librosa.display.specshow(audio_db, sr=samplerate, fmax=8000, ax=ax,\n",
        "                                 x_axis='time', y_axis='mel')\n",
        "\n",
        "  fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "  ax.set(title=title)\n",
        "\n",
        "  if save:\n",
        "    fig.savefig(f'/content/output/{title}.png')\n",
        "\n",
        "\n",
        "def recover_audio(melspec):\n",
        "  \"\"\"Turn a mel-spectrogram back to audio using Griffim-Lim algorithm.\"\"\"\n",
        "  stft = librosa.feature.inverse.mel_to_stft(melspec, sr=SAMPLING_RATE)\n",
        "  audio = librosa.griffinlim(stft)\n",
        "  return audio"
      ],
      "metadata": {
        "id": "tpCVoMtTgUYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "In this section, we will prepare a TensorFlow Dataset object which can be fitted to the model for training. To achieve this, we will load a pair of two lists of audio, one contains the inputs, while the other contains the corresponding groundtruths to be generated. The loaded WAV audios will then be converted to mel-spectrograms and normalized to [-1, 1]."
      ],
      "metadata": {
        "id": "GI61squ0OOAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the training data.\n",
        "INPUT_PATH = '/content/drive/MyDrive/cuhk/fyp/jsb/piano_wav'\n",
        "GROUNDTRUTH_PATH = '/content/drive/MyDrive/cuhk/fyp/jsb/violin_wav'"
      ],
      "metadata": {
        "id": "4xtXa2p-duoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio I/O settings, values obtained from manual experiment.\n",
        "SAMPLING_RATE = 22050\n",
        "DURATION = 5.94"
      ],
      "metadata": {
        "id": "MqXGQj31f0bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our training set consist of 305 images.\n",
        "BUFFER_SIZE = 305\n",
        "# Value of 1 gives better results for U-Net in original pix2pix experiment.\n",
        "BATCH_SIZE = 1\n",
        "# Each spectrogram image is 256x256x1 in size.\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "OUTPUT_CHANNELS = 1"
      ],
      "metadata": {
        "id": "pbXjk2iMRZHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all(dir):\n",
        "  \"\"\"Load all WAV files inside a directory into a list of training images.\"\"\"\n",
        "  data = []\n",
        "  for i, p in enumerate(Path(dir).glob('*.wav')):\n",
        "    data.append(p)\n",
        "  return [load_audio(p) for p in data]\n",
        "\n",
        "\n",
        "def load_audio(path, duration=DURATION, samplerate=SAMPLING_RATE):\n",
        "  \"\"\"Load a WAV file into a normalised training spectrogram image.\"\"\"\n",
        "  audio, samplerate = librosa.load(path, duration=duration, sr=samplerate)\n",
        "  spec = librosa.feature.melspectrogram(y=audio, sr=samplerate, n_mels=256)\n",
        "  normalized = normalize(spec)\n",
        "  expanded = np.expand_dims(normalized, axis=2)  # (256, 256, 1)\n",
        "  return expanded\n",
        "\n",
        "\n",
        "def normalize(spec):\n",
        "  \"\"\"Normalize a spectrogram to [-1, 1].\"\"\"\n",
        "  spec = (spec - 127.5) / 127.5\n",
        "  return spec\n",
        "\n",
        "\n",
        "def denormalize(spec):\n",
        "  \"\"\"Denormalize a spectrogram to [0, 255].\"\"\"\n",
        "  spec = spec * 127.5 + 127.5\n",
        "  return spec\n",
        "\n",
        "\n",
        "def tensor_to_spec(tensor):\n",
        "  \"\"\"Turn model tensor (1, 256, 256, 1) to mel-spectrogram (256, 256).\"\"\"\n",
        "  return denormalize(np.array(tensor[0, ...]).squeeze())\n",
        "\n",
        "\n",
        "def prepare_dataset(test_ratio=10):\n",
        "  \"\"\"\n",
        "  Prepare a pair of training and testing TensorFlow Dataset objects.\n",
        "  \n",
        "  Args:\n",
        "    test_ratio (int): percentage split for test set, 10 stands for 10%.\n",
        "\n",
        "  Returns:\n",
        "    Two TensorFlow Dataset objects, one for training another for testing.\n",
        "  \"\"\"\n",
        "  # Load files from input and groundtruth directories respectively.\n",
        "  input_specs = load_all(INPUT_PATH)\n",
        "  print(f'{len(input_specs)} input wav files loaded.')\n",
        "  groundtruth_specs = load_all(GROUNDTRUTH_PATH)\n",
        "  print(f'{len(groundtruth_specs)} groundtruth wav files loaded.')\n",
        "  assert len(input_specs) == len(groundtruth_specs)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_specs, groundtruth_specs))\n",
        "\n",
        "  # Split train-test sets from original dataset.\n",
        "  test_dataset = dataset.take(len(input_specs) // test_ratio)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "  train_dataset = dataset.skip(len(input_specs) // test_ratio)\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "  train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "  return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "vQYPfN2Lz0PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = prepare_dataset()"
      ],
      "metadata": {
        "id": "70NaD90UW2q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "_iBT2r6Vl-07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator"
      ],
      "metadata": {
        "id": "s3-jMLMiexIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "hn6pbQLmmJhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "ZzN5dXEsmLFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "    downsample(128, 4),                        # (batch_size, 64, 64, 128)\n",
        "    downsample(256, 4),                        # (batch_size, 32, 32, 256)\n",
        "    downsample(512, 4),                        # (batch_size, 16, 16, 512)\n",
        "    downsample(512, 4),                        # (batch_size, 8, 8, 512)\n",
        "    downsample(512, 4),                        # (batch_size, 4, 4, 512)\n",
        "    downsample(512, 4),                        # (batch_size, 2, 2, 512)\n",
        "    downsample(512, 4),                        # (batch_size, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
        "    upsample(512, 4),                      # (batch_size, 16, 16, 1024)\n",
        "    upsample(256, 4),                      # (batch_size, 32, 32, 512)\n",
        "    upsample(128, 4),                      # (batch_size, 64, 64, 256)\n",
        "    upsample(64, 4),                       # (batch_size, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model.\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections.\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "-8r5RonOo5pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "metadata": {
        "id": "OtabvWcAo7df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the generator.\n",
        "for example_input, example_target in test_dataset.take(1):\n",
        "  pred = generator(example_input, training=False)\n",
        "  plot_spec(tensor_to_spec(pred), samplerate=SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "Jx5wfmQRo8qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAMBDA = 100"
      ],
      "metadata": {
        "id": "59Dz26jqpI9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "ZZjpOqRPFfEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))  # MAE\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "metadata": {
        "id": "0ucu8n3NFgAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "mnTDXM_Ne2uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar])                               # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x)                                       # (batch_size, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1)                                         # (batch_size, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2)                                         # (batch_size, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)                        # (batch_size, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)                  # (batch_size, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)                   # (batch_size, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "metadata": {
        "id": "eETt1dmoFhFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "metadata": {
        "id": "m4G60HMMFipA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "  return total_disc_loss"
      ],
      "metadata": {
        "id": "rgoQ07plFlI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "fqCgQXYsiMCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "metadata": {
        "id": "6ybbiZxCFrmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "KxzVN7THFtQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images(model, input, groundtruth, epoch):\n",
        "  pred = model(input, training=True)\n",
        "\n",
        "  input_spec = tensor_to_spec(input)\n",
        "  truth_spec = tensor_to_spec(groundtruth)\n",
        "  pred_spec = tensor_to_spec(pred)\n",
        "\n",
        "  plot_spec(input_spec, title=f'Input', samplerate=SAMPLING_RATE)\n",
        "  plot_spec(truth_spec, title=f'Ground Truth', samplerate=SAMPLING_RATE)\n",
        "  plot_spec(pred_spec, title=f'Epoch {epoch} Prediction', samplerate=SAMPLING_RATE)\n",
        "  \n",
        "  return input_spec, truth_spec, pred_spec"
      ],
      "metadata": {
        "id": "pXjR-tlbFuR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_input, example_target in test_dataset.take(1):\n",
        "  generate_images(generator, example_input, example_target, 0)"
      ],
      "metadata": {
        "id": "YLNbTsT1GOGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, step):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
      ],
      "metadata": {
        "id": "UFelIX_gJQei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "  example_input, example_target = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "\n",
        "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "    # Generate an image every 1000 steps.\n",
        "    if (step) % 1000 == 0:\n",
        "      IPython.display.clear_output(wait=True)\n",
        "      print(f\"Step: {step//1000}k\")\n",
        "      if step != 0:\n",
        "        print(f'Time taken for last 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "      start = time.time()\n",
        "      generate_images(generator, example_input, example_target, step)\n",
        "\n",
        "    train_step(input_image, target, step)\n",
        "    \n",
        "    # Print a dot every 10 steps.\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.', end='', flush=True)\n",
        "\n",
        "    # Save (checkpoint) the model every 5k steps.\n",
        "    if (step + 1) % 5000 == 0:\n",
        "      checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "metadata": {
        "id": "vtF00z59JaCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir=\"logs/\"\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "metadata": {
        "id": "7i5_SsC9GPo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "metadata": {
        "id": "JPh1tK_9Jchb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(train_dataset, test_dataset, steps=40000)"
      ],
      "metadata": {
        "id": "UoA7dsMeJet2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "BXPrtDNugUjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all saved checkpoints.\n",
        "!ls {checkpoint_dir}"
      ],
      "metadata": {
        "id": "totZYPfEJqLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To restore the latest checkpoint, uncomment the line below.\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# To restore a specific checkpoint, uncomment the line below.\n",
        "# checkpoint.restore('/content/training_checkpoints/ckpt-7')"
      ],
      "metadata": {
        "id": "21asuys-NgZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the trained model on a few examples from the test set.\n",
        "for inp, tar in test_dataset.take(1):\n",
        "  _, _, pred = generate_images(generator, inp, tar, 39000)\n",
        "  recovered = recover_audio(pred)"
      ],
      "metadata": {
        "id": "J8GfMrLoNiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's hear the recovered audio of the predicted spectrogram.\n",
        "IPython.display.Audio(data=recovered, rate=SAMPLING_RATE)"
      ],
      "metadata": {
        "id": "sx3gXzEzOwMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exports"
      ],
      "metadata": {
        "id": "zPHICqCuvsjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPORT_PATH = 'export'"
      ],
      "metadata": {
        "id": "rbD583o7yIjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(generator, EXPORT_PATH)"
      ],
      "metadata": {
        "id": "ciBCLMkNvvVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}